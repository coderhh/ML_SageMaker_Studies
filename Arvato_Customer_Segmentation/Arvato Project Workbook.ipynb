{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "#azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';',low_memory=False)\n",
    "customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    95145\n",
       "NaN    49927\n",
       "3.0    23655\n",
       "8.0    20475\n",
       "2.0     2057\n",
       "4.0      251\n",
       "6.0      142\n",
       "Name: GEBAEUDETYP, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.GEBAEUDETYP.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start from the `azdias` data, first show the summarized info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azdias.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(azdias.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first glance, there are 366 cloumns in the `azdias` dataset, which means 366 attributes, and lots of missing values (`NaN`) in each attributes. I think the first step is to fingure out what does each attribute mean.\n",
    "\n",
    "After review the `DIAS Attributes - Values 2017.xlsx` file, I found some attributes are not well explained, such as `AKT_DAT_KL`, also some attributes has missing values which is not `NaN`, such as `-1` in `AGER_TYP`. \n",
    "\n",
    "So I parsed a `.csv` from `DIAS Attributes - Values 2017.xlsx`, which lists all missing values for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_missing_value(inputPath, outputPath):\n",
    "    # load the attrbutes values summary .xlsx file\n",
    "    attributes_values = pd.read_excel(inputPath, sheet_name='Tabelle1')\n",
    "    \n",
    "    \n",
    "    # drop first meaningless column\n",
    "    attributes_values.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    \n",
    "    # get all attributes names with description and value meaning:\n",
    "    attributes_names = attributes_values.dropna()[['Attribute','Description','Meaning']]\n",
    "\n",
    "    # get all attributes with unknow meaning\n",
    "    attributes_values_unknown = attributes_values[attributes_values.Meaning.isin(['unknown','unknown / no main age detectable'])][['Attribute','Value']]\n",
    "    \n",
    "    \n",
    "    # there are attributes which is special 'RELAT_AB' 'KBA05_AUTOQUOT' 'CAMEO_DEUG_2015'\n",
    "    attributes_values_unknown.dropna(inplace=True)\n",
    "    \n",
    "    #attributes_values_unknown[attributes_values_unknown.Attribute == 'CAMEO_DEUG_2015'].Value = '-1,X'\n",
    "    attributes_values_unknown.loc[len(attributes_values_unknown)] = ['RELAT_AB','-1,9']\n",
    "    \n",
    "    attributes_values_unknown.loc[len(attributes_values_unknown)] = ['KBA05_AUTOQUOT','-1,9']\n",
    "    \n",
    "    # Merge attributes_names with attributes_values_unknown\n",
    "    attributes_missing_values = pd.merge(attributes_names,attributes_values_unknown,on = ['Attribute'],how = 'outer')\n",
    "    print(attributes_missing_values.shape)\n",
    "    # rename the Value column to Missing Value\n",
    "    attributes_missing_values.rename(columns={\"Value\": \"Missing Value\"},inplace=True)\n",
    "    \n",
    "    # save \n",
    "    attributes_missing_values.to_csv(outputPath,index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'DIAS Attributes - Values 2017.xlsx'\n",
    "outfile = 'attributes_missing_values.csv'\n",
    "summary_missing_value(file, outfile)\n",
    "attributes_missing_values = pd.read_csv(outfile)\n",
    "attributes_missing_values_attributes = attributes_missing_values.Attribute.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can use this `.csv` file with the `azdias` dataset to convert all missing\\unknown value to `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_copy = azdias.copy()\n",
    "azdias_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all attributes name\n",
    "column_names = azdias_copy.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_list_func(attribute_name):\n",
    "    '''\n",
    "    Get the missing_or_unknown value list from attributes_missing_values.\n",
    "    \n",
    "    <INPUT>\n",
    "    attribute_name: attribute name in attributes_missing_values    \n",
    "    \n",
    "    <OUTPUT>\n",
    "    missing_value_list:  the string value list that should be substituted as np.nan\n",
    "    \n",
    "    '''\n",
    "    missing_str = str(attributes_missing_values[attributes_missing_values['Attribute'] == attribute_name]['Missing Value'].iloc[0]).replace(' ', '')\n",
    "    missing_value_list = missing_str.split(',')   \n",
    "    \n",
    "    return missing_value_list\n",
    "\n",
    "\n",
    "def int_float_fix(obj):\n",
    "    '''\n",
    "    Purpose: for an object that is float type, return the string integer to assure the substituting process works well.\n",
    "    For the other types, return the string type. (nan is remain as nan)\n",
    "    Ex: \n",
    "    3.0 -> '3'\n",
    "    5.0 -> '5'\n",
    "    nan -> nan\n",
    "    'XX' -> 'XX'\n",
    "    2 -> '2'\n",
    "    \n",
    "    <INPUT>\n",
    "    objects with the following types: int, float, str\n",
    "    \n",
    "    <OUTPUT>\n",
    "    str object\n",
    "    \n",
    "    '''\n",
    "    if type(obj) == str or pd.isnull(obj):\n",
    "        return obj\n",
    "    else:\n",
    "        return str(int(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for column_name in column_names:\n",
    "    if column_name in attributes_missing_values_attributes:\n",
    "        i = i + 1 \n",
    "        print(i, column_name) # debug\n",
    "        missing_value_list = nan_list_func(column_name)\n",
    "        azdias_copy[column_name] = azdias_copy[column_name].map(lambda x: np.nan if int_float_fix(x) in missing_value_list else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_copy.to_csv('azdias_with_all_nans.csv', index = False)\n",
    "#azdias_copy = pd.read_csv('azdias_with_alll_nans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caculate NaN ratio of each columns\n",
    "nan_ratio_column_wise = (azdias_copy.isnull().sum() / len(azdias_copy)).sort_values(ascending=False)\n",
    "nan_ratio_column_wise.hist()\n",
    "\n",
    "plt.xlabel(\"NaN ratio\")\n",
    "plt.ylabel(\"number of columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histgram, most of the columns have less than 30% of missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_column_names = nan_ratio_column_wise[nan_ratio_column_wise.values > 0.3].index\n",
    "outlier_column_names\n",
    "# Delete the outlier columns that have more than 30% missing data\n",
    "column_cleaned_azdias = azdias_copy.drop(outlier_column_names, axis = 1)\n",
    "column_cleaned_azdias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are 356 attributes left in the dataset after the drop. In order to proceed to data engineering and transforming step, all attributes should be assigned to the following types: `categorical`, `numerical`, `ordinal`, `binary`. For this step, I have manually created `attributes_type_merged_action.csv` file with attribute names, types, action (keep, drop, one-hot or engineering)(keep is default NaN).\n",
    "\n",
    "Since not all of the columns were provided with description,To make it simple, I will drop all the columns which are not explained in the description file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias.to_csv('column_cleaned_azdias.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_cleaned_azdias = pd.read_csv('column_cleaned_azdias.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_column_names = column_cleaned_azdias.columns.values\n",
    "notexplained_columns = []\n",
    "for column_name in column_cleaned_azdias_column_names:\n",
    "    if column_name not in attributes_missing_values_attributes:\n",
    "        notexplained_columns.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained = column_cleaned_azdias.drop(columns=notexplained_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_cleaned_azdias_without_notexplained.to_csv('column_cleaned_azdias_without_notexplained.csv', index=None)\n",
    "column_cleaned_azdias_without_notexplained = pd.read_csv('column_cleaned_azdias_without_notexplained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_attributes_type(attributes_type, attributes_type_merged):\n",
    "    left_column_df = pd.DataFrame(column_cleaned_azdias_without_notexplained.columns.values, columns=['Attribute'])\n",
    "    attribute_type_df = pd.read_csv(attributes_type)\n",
    "    attribute_type_df['Action'] = ''\n",
    "    merged_df = left_column_df.merge(attribute_type_df, how='left')\n",
    "    merged_df.to_csv(attributes_type_merged, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_type_merged_action = pd.read_csv('attributes_type_merged_action.csv')\n",
    "attributes_type_merged_action.Action.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop all the columns which are marked as `drop` in the`action` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the attribute names which need to be dropped\n",
    "drop_columns = attributes_type_merged_action[attributes_type_merged_action.Action == 'drop'].Attribute.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop attributes\n",
    "column_cleaned_azdias_without_notexplained = column_cleaned_azdias_without_notexplained.drop(columns=drop_columns)\n",
    "column_cleaned_azdias_without_notexplained.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform columns which are marked as `engineer` in the `action` column\n",
    "\n",
    "There are several attributes which can be re-engineered to new attributes: `OST_WEST_KZ`, `PRAEGENDE_JUGENDJAHRE`, `WOHNLAGE`,`PLZ8_BAUMAX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_cleaned_azdias_without_notexplained.to_csv('column_cleaned_azdias_without_notexplained.csv', index=None)\n",
    "#column_cleaned_azdias_without_notexplained = pd.read_csv('column_cleaned_azdias_without_notexplained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the attribute names which need to be onehot encode\n",
    "engineer_columns = attributes_type_merged_action[attributes_type_merged_action.Action == 'engineer'].Attribute.values\n",
    "engineer_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. OST_WEST_KZ feature was encoded as 0 for Ost and 1 for West moving pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained.OST_WEST_KZ.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OST_WEST_KZ_Engineer(x):\n",
    "    x = str(x)\n",
    "    if x == 'W':\n",
    "        return 1\n",
    "    elif x == \"O\":\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained.OST_WEST_KZ = column_cleaned_azdias_without_notexplained.OST_WEST_KZ.apply(OST_WEST_KZ_Engineer)\n",
    "column_cleaned_azdias_without_notexplained.OST_WEST_KZ.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. PRAEGENDE_JUGENDJAHRE —> MOVEMENT (1: Mainstream, 2: Avantgarde) and GENERATION_DECADE (4: 40s, 5: 50s, 6: 60s, 7: 70s, 8: 80s, 9: 90s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained.PRAEGENDE_JUGENDJAHRE.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRAEGENDE_JUGENDJAHRE_MOVEMENT_Engineer(x):\n",
    "    if x in (1,3,5,8,10,12,14):\n",
    "        return 1\n",
    "    elif x in (2,4,6,7,11,13,15):\n",
    "        return 2\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def PRAEGENDE_JUGENDJAHRE_GENERATION_DECADE_Engineer(x):\n",
    "    if x in (1,2):\n",
    "        return 4\n",
    "    elif x in (3,4):\n",
    "        return 5\n",
    "    elif x in (5,6,7):\n",
    "        return 6\n",
    "    elif x in (8,9):\n",
    "        return 7\n",
    "    elif x in (10,11,12,13):\n",
    "        return 8\n",
    "    elif x in (14,15):\n",
    "        return 9\n",
    "    else:\n",
    "        return np.nan        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement = column_cleaned_azdias_without_notexplained.PRAEGENDE_JUGENDJAHRE.apply(PRAEGENDE_JUGENDJAHRE_MOVEMENT_Engineer)\n",
    "movement.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_decade = column_cleaned_azdias_without_notexplained.PRAEGENDE_JUGENDJAHRE.apply(PRAEGENDE_JUGENDJAHRE_GENERATION_DECADE_Engineer)\n",
    "generation_decade.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained['MOVEMENT'] = movement\n",
    "column_cleaned_azdias_without_notexplained['GENERATION_DECADE'] = generation_decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. WOHNLAGE —> RURAL_NEIGBORHOOD (0: Not Rural, 1: Rural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained.WOHNLAGE.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WOHNLAGE_RURAL_NEIGBORHOOD_engineer(x):\n",
    "    if x in (0,1,2,3,4,5):\n",
    "        return 0\n",
    "    elif x in (7,8):\n",
    "        return 1\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rural_neighboorhood = column_cleaned_azdias_without_notexplained.WOHNLAGE.apply(WOHNLAGE_RURAL_NEIGBORHOOD_engineer)\n",
    "rural_neighboorhood.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained['RURAL_NEIGBORHOOD'] = rural_neighboorhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. PLZ8_BAUMAX —> PLZ8_BAUMAX_FAMILY (0: 0 families, 1: mainly 1–2 family homes, 2: mainly 3–5 family homes, 3: mainly 6–10 family homes, 4: mainly 10+ family homesand PLZ8_BAUMAX_BUSINESS (0: Not Business, 1: Business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained.PLZ8_BAUMAX.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLZ8_BAUMAX_PLZ8_BAUMAX_FAMILY_Engineer(x):\n",
    "    if x in (1,2,3,4):\n",
    "        return x\n",
    "    elif x == 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def PLZ8_BAUMAX_PLZ8_BAUMAX_BUSINESS_Engineer(x):\n",
    "    if x in (1,2,3,4):\n",
    "        return 0\n",
    "    elif x == 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family = column_cleaned_azdias_without_notexplained.PLZ8_BAUMAX.apply(PLZ8_BAUMAX_PLZ8_BAUMAX_FAMILY_Engineer)\n",
    "family.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = column_cleaned_azdias_without_notexplained.PLZ8_BAUMAX.apply(PLZ8_BAUMAX_PLZ8_BAUMAX_BUSINESS_Engineer)\n",
    "business.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained['PLZ8_BAUMAX_FAMILY'] = family\n",
    "column_cleaned_azdias_without_notexplained['PLZ8_BAUMAX_BUSINESS'] = business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the original attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained_engineered = column_cleaned_azdias_without_notexplained.drop(columns=['PRAEGENDE_JUGENDJAHRE','PLZ8_BAUMAX','WOHNLAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained_engineered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_cleaned_azdias_without_notexplained_engineered.to_csv('column_cleaned_azdias_engineered.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained_engineered = pd.read_csv('column_cleaned_azdias_engineered.csv')\n",
    "column_cleaned_azdias_without_notexplained_engineered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Onehot encode all categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all `cat_attributes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the attribute names which need to be onehot encode, these are categorical attributes from original dataset\n",
    "onehot_columns = attributes_type_merged_action[attributes_type_merged_action.Action == 'onehot'].Attribute.values\n",
    "# combine with the new categorical attributes from feature engineer\n",
    "cat_attributes = list(onehot_columns)\n",
    "cat_attributes.extend(('MOVEMENT','GENERATION_DECADE','PLZ8_BAUMAX_FAMILY'))\n",
    "cat_attributes = np.array(cat_attributes,dtype='object')\n",
    "cat_attributes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all `bin_attributes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get binary columns from the original dataset\n",
    "binary_columns = attributes_type_merged_action[attributes_type_merged_action.Type == 'binary'].Attribute.values\n",
    "# combine with new binary attributes from feature egineer\n",
    "bin_attributes = list(binary_columns)\n",
    "bin_attributes.append('PLZ8_BAUMAX_BUSINESS')\n",
    "bin_attributes.append('RURAL_NEIGBORHOOD')\n",
    "bin_attributes = np.array(bin_attributes,dtype='object')\n",
    "bin_attributes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all `num_attributes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numeric and orinal attributes\n",
    "num_attributes = attributes_type_merged_action[attributes_type_merged_action.Action.isnull()].Attribute.values\n",
    "num_attributes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are `X` in `CAMEO_DEUG_2015`, also there int and floats, I need make them consitent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_NaN(x):\n",
    "    if x == 'X':\n",
    "        return np.nan\n",
    "    elif np.isnan(float(x)):\n",
    "        return x\n",
    "    else:\n",
    "        return int(x)\n",
    "column_cleaned_azdias_without_notexplained_engineered.CAMEO_DEUG_2015=column_cleaned_azdias_without_notexplained_engineered.CAMEO_DEUG_2015.apply(X_NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cleaned_azdias_without_notexplained_engineered.RURAL_NEIGBORHOOD.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with nans\n",
    "#del azdias_df\n",
    "azdias_df = column_cleaned_azdias_without_notexplained_engineered.copy()\n",
    "for column in azdias_df.columns:\n",
    "    if column in cat_attributes:\n",
    "        azdias_df[column].fillna(0, inplace=True)\n",
    "    elif column in bin_attributes:\n",
    "        azdias_df[column].fillna(azdias_df[column].value_counts().index[0], inplace=True)\n",
    "    elif column in num_attributes:\n",
    "        #print(column)\n",
    "        azdias_df[column].fillna(azdias_df[column].median(), inplace=True)\n",
    "\n",
    "azdias_df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After replace `NaN`, there are still different transformation for different attributes, for example, `cat_attributes` needs one-hot encoding, `num_attribues` needs scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummy datafram for all cat_attributes\n",
    "dummies = pd.concat([pd.get_dummies(azdias_df[col],prefix=col) for col in cat_attributes], axis=1)\n",
    "# combine with original dataset\n",
    "azdias_df = pd.concat([azdias_df, dummies], axis=1)\n",
    "# drop original cat_attributes\n",
    "azdias_df = azdias_df.drop(columns=cat_attributes)\n",
    "azdias_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize all num_attributes\n",
    "scaler = MinMaxScaler()\n",
    "azdias_df[num_attributes] = scaler.fit_transform(azdias_df[num_attributes])\n",
    "azdias_df[num_attributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to preprocess and transform data\n",
    "1. Replace all missing or unknown values from the description files in the dataset with `NaN`, manually create reference file `attributes_missing_values.csv`\n",
    "2. Drop all attributes which NaN ratio is greater than 30\n",
    "3. Drop all attributes which are not in the description file\n",
    "4. Feature engineer for different attribtues: NaN, Onehot Encode, Scale, manually create reference file`attributes_type_merged_action.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all steps and functions to an external python code file `data_preprocess.py`. It could be used on both azdias and customers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset data/Udacity_AZDIAS_052018.csv......\n",
      "\n",
      "Loading reference file attributes_missing_values.csv......\n",
      "\n",
      "Replacing missing/unkonwn values with NaNs and drop attributes which are not in description file......\n",
      "\n",
      "Replacing X with NaN for attribtue CAMEO_DEUG_2015......\n",
      "\n",
      "Dropping attributes NaN ratio > 0.3.......\n",
      "\n",
      "Loading reference file attributes_type_merged_action.csv......\n",
      "\n",
      "Dropping......\n",
      "\n",
      "Feature engineer: OST_WEST_KZ feature was encoded as 0 for Ost and 1 for West moving pattern.\n",
      "\n",
      "Feature engineer: PRAEGENDE_JUGENDJAHRE —> MOVEMENT (1: Mainstream, 2: Avantgarde) and GENERATION_DECADE (4: 40s, 5: 50s, 6: 60s, 7: 70s, 8: 80s, 9: 90s)\n",
      "\n",
      "feature engineer: WOHNLAGE —> RURAL_NEIGBORHOOD (0: Not Rural, 1: Rural)\n",
      "\n",
      "PLZ8_BAUMAX —> PLZ8_BAUMAX_FAMILY (0: 0 families, 1: mainly 1–2 family homes, 2: mainly 3–5 family homes, 3: mainly 6–10 family homes, 4: mainly 10+ family homesand PLZ8_BAUMAX_BUSINESS (0: Not Business, 1: Business)\n",
      "\n",
      "Imputing NaN in left attributes......\n",
      "\n",
      "One hot encoding all categorical attributes......\n",
      "\n",
      "Normalizing all num attributes......\n",
      "\n",
      "Final cleaned dataset shape: (891221, 400)\n",
      "Loading dataset data/Udacity_CUSTOMERS_052018.csv......\n",
      "\n",
      "Loading reference file attributes_missing_values.csv......\n",
      "\n",
      "Replacing missing/unkonwn values with NaNs and drop attributes which are not in description file......\n",
      "\n",
      "Replacing X with NaN for attribtue CAMEO_DEUG_2015......\n",
      "\n",
      "Dropping attributes NaN ratio > 0.3.......\n",
      "\n",
      "Loading reference file attributes_type_merged_action.csv......\n",
      "\n",
      "Dropping......\n",
      "\n",
      "Feature engineer: OST_WEST_KZ feature was encoded as 0 for Ost and 1 for West moving pattern.\n",
      "\n",
      "Feature engineer: PRAEGENDE_JUGENDJAHRE —> MOVEMENT (1: Mainstream, 2: Avantgarde) and GENERATION_DECADE (4: 40s, 5: 50s, 6: 60s, 7: 70s, 8: 80s, 9: 90s)\n",
      "\n",
      "feature engineer: WOHNLAGE —> RURAL_NEIGBORHOOD (0: Not Rural, 1: Rural)\n",
      "\n",
      "PLZ8_BAUMAX —> PLZ8_BAUMAX_FAMILY (0: 0 families, 1: mainly 1–2 family homes, 2: mainly 3–5 family homes, 3: mainly 6–10 family homes, 4: mainly 10+ family homesand PLZ8_BAUMAX_BUSINESS (0: Not Business, 1: Business)\n",
      "\n",
      "Imputing NaN in left attributes......\n",
      "\n",
      "One hot encoding all categorical attributes......\n",
      "\n",
      "Normalizing all num attributes......\n",
      "\n",
      "Final cleaned dataset shape: (191652, 397)\n"
     ]
    }
   ],
   "source": [
    "import data_preprocess as data_pre\n",
    "\n",
    "cleaned_azdias = data_pre.data_preprocess('azdias','data/Udacity_AZDIAS_052018.csv',\n",
    "                                          'attributes_missing_values.csv',\n",
    "                                          'attributes_type_merged_action.csv')\n",
    "\n",
    "cleaned_customer = data_pre.data_preprocess('customer','data/Udacity_CUSTOMERS_052018.csv',\n",
    "                                          'attributes_missing_values.csv',\n",
    "                                          'attributes_type_merged_action.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get two cleaned datasets: `cleaned_azdias` and `cleaned_customer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv Udacity_MAILOUT_052018_TRAIN.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mailout_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(mailout_train.ALTER_KIND1 is nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer – this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv Udacity_MAILOUT_052018_TEST.csv "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
